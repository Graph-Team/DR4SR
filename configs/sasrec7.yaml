model:
  hidden_size: 128
  layer_num: 2
  head_num: 2
  dropout_rate: 0.5
  activation: 'gelu'
  layer_norm_eps: 1.0e-12
  uniformity: 2
  K: 75000
  depth: 8
  H: 5
  ab-wo-pattern: false
  quantization: false


train:
  warmup_epoch: -1
  # learning_rate: 2.e-3

eval:
  batch_size: 512