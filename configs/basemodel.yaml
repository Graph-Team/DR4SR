# Training
batch_size: 512
early_stop_mode: max
early_stop_patience: 40
epochs: 1000
device: 1
optimizer: adam
learning_rate: 0.001
weight_decay: 0
num_neg: 1   # number of negative items to be sampled
seed: 2024  # random seed, usually 42 is a magic number

# Model
embed_dim: 64
loss_fn: 'bce'

# Eval
eval_batch_size: 1024
cutoff: [20, 10, 5]
val_metrics: [ndcg, recall]
test_metrics: [ndcg, recall]
topk: 100 # Max len of user hist to reduce complexity
save_path: './saved/'
