train: 
  batch_size: 256
  early_stop_mode: max
  early_stop_patience: 5
  epochs: 1000
  device: 1
  optimizer: adam
  learning_rate: 0.001
  weight_decay: 0
  num_neg: 1   # number of negative items to be sampled
  seed: 2023  # random seed, usually 42 is a magic number

model: 
  embed_dim: 64
  loss_fn: 'bce'

eval:
  batch_size: 2048
  cutoff: [20, 10]
  val_metrics: [ndcg, recall]
  test_metrics: [ndcg, recall]
  topk: 100 # Max len of user hist to reduce complexity
  save_path: './saved/'
